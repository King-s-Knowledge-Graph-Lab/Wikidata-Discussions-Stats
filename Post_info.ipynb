{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook extracts information for posts and creates csv files in order to take descriptive statistics for the Wikidata discussions\n",
    "\n",
    "Use as input the csv file with the posts produced with Raw_data_analysis.ipynb\n",
    "\n",
    "## 1. Extract username and timestamp for the posts\n",
    "\n",
    "In the raw Wikidata data, in the end of every posts there is a signature username and a timestamp. The username should follow a structure like \"[[User: username | username ]]. However, this is not alwas the case so we create a set of different pattern to detect the usernames. If there is no username signature at the end of the post the code gives an \"Anonymous_username_#\" name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#extract the name form the string based on the pattern.\n",
    "#the flag stops the detection of the name if the pattern works\n",
    "def extract_username(pattern, post,name,flag,loc): \n",
    "    names=[]\n",
    "    if flag==1:\n",
    "        usernames=pattern.finditer(post)\n",
    "        #search for the username        \n",
    "        for match in usernames:\n",
    "                #print(match.group(2))\n",
    "                names.append([match.group(loc),match.end()])\n",
    "\n",
    "        if names!=[]:\n",
    "            flag=0\n",
    "            #choose the username based the location. We choose the last found\n",
    "            loc=0\n",
    "            for j in names:\n",
    "                if loc<j[1]:\n",
    "                    name=j[0]\n",
    "                    loc=j[1]\n",
    "        \n",
    "\n",
    "    return name, flag\n",
    "\n",
    "#create patterns and iterates to detect the name\n",
    "def find_usernames(post, subject,anonymous_num ):\n",
    "    name=[]\n",
    "    if '[[m:Global message delivery' in post:\n",
    "        name='Global message'\n",
    "    elif 'New Wikipedia Library' in subject:\n",
    "        name='Global message'\n",
    "    elif 'The Signpost' in subject:\n",
    "        name='Global message'\n",
    "    elif '[[m:Special:MyLanguage' in subject:\n",
    "        name='Global message'\n",
    "    else:\n",
    "        #create the patterns to detect the usernames\n",
    "        pattern_1=re.compile(r\"(\\[)?(User:|User talk:|user:|user talk:|User Talk:|User_talk:|User_Talk:|user_talk:|Utente:|Usuário:|Utilisateur:|Utilizator:|Usuario discusión:|Usuario:|Usuario Discusi\\u00f3n:|kullanıcı:|Kullan\\u0131c\\u0131:|:USER TALK:|:USER:|U:|u:|Benutzer:|Benutzer Diskussion:|Special:Contributions/|welcominguser=|Discussão:)(.*?)(\\||/|\\])\") # the regular expression I need to extract \n",
    "        pattern_2=re.compile(r\"(\\-)([\\w\\s\\d!\\(\\)\\-\\{\\};:\\'\\\"\\<\\>\\.\\?@#\\$%\\^&\\*_~]+)(\\(?talk\\)? )?(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))$\")\n",
    "        pattern_3=re.compile(r\"(\\-\\-)([\\w\\s\\d!\\(\\)\\-\\{\\};:\\'\\\"\\<\\>\\.\\?@#\\$%\\^&\\*_~]+)(\\(?talk\\)? )?(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))$\")\n",
    "        pattern_4=re.compile(r\"(\\.|!|\\?|\\u3001|\\u3002|,)([\\w\\s\\d\\(\\)\\-\\{\\};:\\'\\\"\\<\\>@#\\$%\\^&\\*_~]+)(\\(?talk\\)? )?(, )?(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))$\")\n",
    "\n",
    "        flag=1# the flag stops the detection if the pattern find a username\n",
    "        name, flag=extract_username(pattern_1,post,name,flag,3)\n",
    "        name, flag=extract_username(pattern_2,post,name,flag,2)\n",
    "        name, flag=extract_username(pattern_3,post,name,flag,2)\n",
    "        name, flag=extract_username(pattern_4,post,name,flag,2)\n",
    "        \n",
    "    #if we could not find a name we asing Anonymous_username_#   \n",
    "    if name==[] or name==' ':\n",
    "        anonymous_num += 1\n",
    "        name='Anonymous_username_'+str(anonymous_num)\n",
    "   \n",
    "    return name,anonymous_num\n",
    "\n",
    "\n",
    "def fill_table(PATH, filename,anonymous_num):\n",
    "    data = pd.read_csv(str(PATH)+str(filename), header=None, encoding='utf-8')\n",
    "    data.columns=['post','location', 'thread_subject', 'discussion_type', 'discussion_page_name']# loose the index number\n",
    "    for row in range(len(data)):\n",
    "        print(row, len(data))\n",
    "        post=data.post[row]\n",
    "        \n",
    "        data.loc[row,'username'], anonymous_num=find_usernames(post,data.thread_subject[row],anonymous_num)\n",
    "        if '(UTC)' in post:\n",
    "            data.loc[row,'timestamp']=re.findall(\"(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))$\", post)[0]\n",
    "        else:data.loc[row,'timestamp']='No date'\n",
    "   \n",
    "    print(anonymous_num)\n",
    "    \n",
    "    #save data to csv\n",
    "    data.to_csv(str(PATH)+'username_timestamp.csv', encoding='utf-8', index=False,  mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======---------> RUN <--------=============\n",
    "\n",
    "PATH='FOLDER_WITH_THE_CSV_INPUT/'\n",
    "filename='filename.csv'\n",
    "\n",
    "anonymous_num=0 # this is to asign an anonymous name to posts without signatures\n",
    "fill_table(PATH, filename ,anonymous_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unique usernames\n",
    "\n",
    "Take as input the csv file with the information of username and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH='FOLDER_WITH_THE_CSV_INPUT/'\n",
    "filename='filename.csv'\n",
    "\n",
    "list_names=pd.read_csv(str(PATH)+str(filename), header=None, encoding='utf-8')\n",
    "list_names.columns=['discussion_page_name','fixed_username']\n",
    "print(len(list_names))\n",
    "\n",
    "data=list_names['fixed_username']\n",
    "name_lst=data.to_list()\n",
    "unique_name_list = list(dict.fromkeys(name_lst))\n",
    "unique_name_df=pd.DataFrame({'username':unique_name_list})\n",
    "print( len(data), len(name_lst), len(unique_name_list), len(unique_name_df))\n",
    "\n",
    "unique_name_df.to_csv(str(PATH)+'unique_usernames.csv', encoding='utf-8', index=False,  mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract username information from Wikidata\n",
    "\n",
    "Extract information like, the number of edits, registration timestamp etc.\n",
    "\n",
    "First create the input txt file. Then query Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('unique_usernames.csv', encoding=\"utf-8\") as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    #extracts the header\n",
    "    #header = next(csvreader)\n",
    "\n",
    "    file_txt=open('editors_sequence.txt','w', encoding=\"utf-8\")\n",
    "    \n",
    "    count_row=0\n",
    "    for row in csvreader:\n",
    "        count_row += 1\n",
    "        #print(row[0])\n",
    "        file_txt.write(row[0])\n",
    "        file_txt.write('|')\n",
    "        print(row[0])\n",
    "    print(count_row)\n",
    "    file_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the info\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://www.wikidata.org/w/api.php\"#info about the features you can extract https://www.wikidata.org/w/api.php?action=help&modules=query%2Busers\n",
    "\n",
    "\n",
    "editors_count=0\n",
    "line_count=0\n",
    "\n",
    "#path to editors names\n",
    "with open('/mnt/data/elisavetk/Theme_2/Post_Graph/editors_sequence.txt', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        \n",
    "        #editors_in_each_line_count=0\n",
    "        for part in l.split(\"|\"):\n",
    "            line_count+=1\n",
    "            #editors_in_each_line_count+=1\n",
    "            \n",
    "            PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"users\",\n",
    "            \"ususers\": part,\n",
    "            # \"usprop\": \"blockinfo|editcount|registration|rights\"} #you choose features to extarct\n",
    "            \"usprop\": \"editcount|registration|rights\"} #you choose features to extarct\n",
    "            \n",
    "\n",
    "            \n",
    "            R = S.get(url=URL, params=PARAMS)\n",
    "            DATA = R.json()\n",
    "            \n",
    "            USERS = DATA[\"query\"][\"users\"]\n",
    "            \n",
    "                        \n",
    "            users = pd.DataFrame(columns=['userid','name','editcount', 'registration','rights'])#create a data frame with the features you choose\n",
    "            #unknown_users=pd.DataFrame(columns=['names'])  \n",
    "            for u in USERS:\n",
    "                editors_count+=1\n",
    "                #if ('invalid' not in u.keys()) and ('missing' not in u.keys()):\n",
    "                    #print(str(u[\"name\"]) + \" has \" + str(u[\"editcount\"]) + \" edits.\")\n",
    "                    #print(str(u[\"name\"]) + ' with id ' + str(u['userid']))\n",
    "                    #print(u['groupmemberships'])\n",
    "                users = users.append(u, ignore_index=True)\n",
    "                #else:unknown_users=unknown_users.append(u, ignore_index=True)\n",
    "                \n",
    "            \n",
    "            #path to save       \n",
    "            users.to_csv('editors_info.csv', encoding='utf-8', index=False,  mode='a', header=False)\n",
    "            #unknown_users.to_csv('/mnt/data/elisavetk/Theme_2/Post_Graph/unknown_editor_info.csv', encoding='utf-8', index=False,  mode='a', header=False)\n",
    "            \n",
    "            #print(editors_count)\n",
    "\n",
    "            '''if editors_in_each_line_count == 49:\n",
    "                break'''\n",
    "        #break\n",
    "\n",
    "\n",
    "print('Number of lines')\n",
    "print(line_count)\n",
    "\n",
    "print('Number of all imported editors')    \n",
    "print(editors_count) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f38612211a5ac91abaa265d09091e9511deaf737e8b839893735c46e6969015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
