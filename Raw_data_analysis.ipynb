{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the raw discussion data in Wikidata\n",
    "\n",
    "## 1. Extract the raw discussion\n",
    "\n",
    "Use [this process](https://github.com/King-s-Knowledge-Graph-Lab/Wikidata-Discussion-Parser) to create csv files with revisions on Wikidata discussion pages. The process creates one csv file for every discussion page. The rows in the files represent the revision of the discussion page.\n",
    "\n",
    "Use the csv files as input to the below process to get:\n",
    "1. the full raw discussion for every file (function import_csv)\n",
    "2. the different discussions on the page (Wikidata discussion pages may include more than one discussion threads, they are separated based on a subject title) (function separate_discussions)\n",
    "3. the different posts for every discussion on the page (function sep_posts)\n",
    "\n",
    "\n",
    "The output of the below process is a json file for every input discussion page, with the different discussions and the different posts.\n",
    "Every element represents a thread. In the thread level, \"subject\" has the title of the discussion (if no title in the discussion it returns 'No subject'), and \"thread\" has a list of the posts in this thread.\n",
    "\n",
    "If there are no threads in the discussion page (some discussion pages include only meta data information like description tables) the process returns empty json files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "#this is to extent the size of the reading csv cell\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "#this function reads the csv and creates a list with the rows\n",
    "def import_csv(csvfilename):\n",
    "    data = []\n",
    "    row_index=0\n",
    "    \n",
    "    with open(csvfilename, \"r\", encoding=\"utf-8\", errors=\"ignore\", header=None) as scraped:\n",
    "        reader = csv.reader(scraped, delimiter=',')\n",
    "        if reader is not None:\n",
    "            for row in reader:\n",
    "                if row:  # avoid blank lines\n",
    "                    row_index += 1\n",
    "                    columns = [row[7]]\n",
    "                    data.append(columns)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#to separate threads with no title into posts\n",
    "def no_title(temp, all):\n",
    "    if (temp[0][0:2]=='{{') and (('documentation' in temp[0]) or ('Documentation' in temp[0])):\n",
    "        a=1 \n",
    "    else:\n",
    "        if ('#REDIRECT' not in temp[0]):\n",
    "            disc_dict={}\n",
    "            if '(UTC)' in temp[0]:\n",
    "                posts_lst=[]\n",
    "                disc_dict['subject']='No subject'\n",
    "                posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',temp[0])\n",
    "                for i in range(0,len(posts)-1, 2):\n",
    "                    posts_lst.append(posts[i]+posts[i+1])\n",
    "    \n",
    "                disc_dict['thread']=posts_lst\n",
    "                all.append(disc_dict) \n",
    "    return all\n",
    "\n",
    "\n",
    "#to separate threads with titles into posts\n",
    "def sep_posts(titles, temp, all):\n",
    "    for j in range(len(temp)):\n",
    "        if (temp[j] in titles):\n",
    "            disc_dict={}\n",
    "            if (j+1)==(len(temp)):\n",
    "                disc_dict['subject']=temp[j]\n",
    "                disc_dict['thread']=\"\"\n",
    "                all.append(disc_dict)\n",
    "            elif '(UTC)' in temp[j+1]:\n",
    "                posts_lst=[]\n",
    "                disc_dict['subject']=temp[j]\n",
    "                posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',temp[j+1])\n",
    "                for i in range(0,len(posts)-1, 2):\n",
    "                    posts_lst.append(posts[i]+posts[i+1])\n",
    "    \n",
    "                disc_dict['thread']=posts_lst\n",
    "                all.append(disc_dict)\n",
    "    return all\n",
    "            \n",
    "\n",
    "#function to create a file with separate threads and separate posts for the discussion page\n",
    "def separate_discussions(file_name, new_file_path, name):\n",
    "\n",
    "    data = import_csv(file_name)#call the csv data\n",
    "    last_row = data[-1]#take the last row including all threads in the discussion page\n",
    "\n",
    "    titles = re.findall('==(.*)==', last_row[0])#find the titles in the discussion  \n",
    "    titles=[s.replace('=', '') for s in titles]\n",
    "    \n",
    "    #titles=list(set(titles)) \n",
    "    temp= last_row[0].split('==')\n",
    "    temp = [x for x in temp if x != '']\n",
    "    temp=[s.strip('=') for s in temp]\n",
    "    \n",
    "    all=[]\n",
    "    if titles==[]:\n",
    "        all=no_title(temp, all) \n",
    "    else:\n",
    "        all=no_title(temp, all)\n",
    "        all=sep_posts(titles, temp, all)\n",
    "\n",
    "    with open(str(new_file_path)+str(name[:-4])+'.json', \"w\") as outfile:\n",
    "        json_object = json.dumps(all,indent=4)                \n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path='JSON_FILE/'\n",
    "initial_folder_path='CSV_FILE/'\n",
    "filenames_folder='LIST_OF_CSV_FILES_TO_PROCESS/'\n",
    "\n",
    "\n",
    "#=======================================================================\n",
    "#========= ------->>>> R U N <<<<-------- ==============================\n",
    "\n",
    "#create a folder to save the new csv files with the edges (two columns with usernames that talk in the same talk pages)            \n",
    "# if not os.path.exists(new_folder_name):\n",
    "#         os.mkdir(new_folder_name)         \n",
    "\n",
    "#read the name of the csv files in the TP_csv_ folder  \n",
    "file1 = open(filenames_folder, 'r') \n",
    "Lines = file1.readlines() \n",
    "\n",
    "\n",
    "\n",
    "#call the csv files from the TP_csv_ folder\n",
    "for i in Lines:\n",
    "    #extarct the space before and after the string name\n",
    "    name=\"\".join(i.split())\n",
    "    print(\"start: \" +str(name))\n",
    "    #check if the csv is empty\n",
    "    if os.stat(str(initial_folder_path)+str(name)).st_size == 0:\n",
    "        continue\n",
    "    #call the function to find the usernames and create the edges\n",
    "    list_usernames=separate_discussions(str(initial_folder_path)+str(name),new_file_path , name)\n",
    "    print(\"finish: \" +str(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract information for the posts in the Wikidata discussions\n",
    "\n",
    "Use the json files produced in the above process to process the posts in the thread. \n",
    "\n",
    "The below code creates a csv with information about the posts. Every row is a posts and the columns include:\n",
    "1. the post\n",
    "2. the place of the post ij the thread (e.g. 1 if it is the first post, 2 if it is the second post etc.) \n",
    "3. the subject title of the thread includes the post\n",
    "4. the name of discussion page includes the post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def extract_info(filename,name,data):\n",
    "    \n",
    "    #read the json with the threads\n",
    "    f=codecs.open(filename,encoding='utf-8', mode='r')\n",
    "    jsonObject=json.load(f)\n",
    "    #read the thread to read the posts\n",
    "    for thread in range(len(jsonObject)):\n",
    "        location=0\n",
    "        if isinstance(jsonObject[thread]['thread'], str):\n",
    "            location +=1 \n",
    "            row={'post':jsonObject[thread]['thread'],'location':location, 'thread_subject':jsonObject[thread]['subject'], 'discussion_page_name':name}\n",
    "            data=data.append(row, ignore_index=True)\n",
    "        else:\n",
    "            for post in jsonObject[thread]['thread']:\n",
    "                # print(repr(post))\n",
    "                location +=1 \n",
    "                row={'post':post,'location':location, 'thread_subject':jsonObject[thread]['subject'], 'discussion_page_name':name}\n",
    "                data=data.append(row, ignore_index=True)\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def posts2csv(PATH_input,PATH_output, filenames):\n",
    "\n",
    "    data = pd.DataFrame(columns=['post','location', 'thread_subject', 'discussion_page_name'])#create a df to save the info\n",
    "\n",
    "    #read the filenames  \n",
    "    file=open(str(PATH_input)+str(filenames), 'r')\n",
    "    lines=file.readlines()\n",
    "    for line in lines:\n",
    "        name=\"\".join(line.split())#remove the space before and after the string name\n",
    "        print(\"start: \" +str(name))\n",
    "        data=extract_info(str(PATH_input)+str(name),name,data)\n",
    "        print(\"finish: \" +str(name))\n",
    "    file.close()\n",
    "\n",
    "    #save data to csv\n",
    "    data.to_csv(str(PATH_output)+'posts.csv', encoding='utf-8', index=False,  mode='a', header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<-------RUN------------->\n",
    "PATH_input='SAVED_JSON_FILES_FOLDER/'\n",
    "PATH_output='FOLDER_TO_SAVE_THE_POST_CSV/'\n",
    "\n",
    "#filenames.txt is a files includes the json names, every row is a filename\n",
    "\n",
    "posts2csv(PATH_input,PATH_output, 'filenames.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f38612211a5ac91abaa265d09091e9511deaf737e8b839893735c46e6969015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
