{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the raw discussion data in Wikidata\n",
    "\n",
    "## 1. Extract the raw discussion\n",
    "\n",
    "Use [this process](https://github.com/King-s-Knowledge-Graph-Lab/Wikidata-Discussion-Parser) to create csv files with revisions on Wikidata discussion pages. The process creates one csv file for every discussion page. The rows in the files represent the revision of the discussion page.\n",
    "\n",
    "Use the csv files as input to the below process to get:\n",
    "1. the full raw discussion for every file (function import_csv)\n",
    "2. the different dicussion on the page (Wikidata discussion pages may include more than one discussion threads, they are separated based on a subject title) (function separate_discussions)\n",
    "3. the different posts for every discussion on the page (function sep_posts)\n",
    "\n",
    "\n",
    "The output of the below process is a jshon file for every input discussion page, with the different discussions and the different posts.\n",
    "Every element represent a thread. in the thread level, \"subject\" has the title of the discussion (if no title in the discussion it returns 'No subject'), and \"thread\" has a list of the posts in this thread.\n",
    "\n",
    "If there is are not threads in the discusion page (some discussion pages include only meta data information like description tables) the process returns and empty json files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "#this is to extent the size of the reading csv cell\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "#this function reads the csv and creates a list with the rows\n",
    "def import_csv(csvfilename):\n",
    "    data = []\n",
    "    row_index=0\n",
    "    \n",
    "    with open(csvfilename, \"r\", encoding=\"utf-8\", errors=\"ignore\", header=None) as scraped:\n",
    "        reader = csv.reader(scraped, delimiter=',')\n",
    "        if reader is not None:\n",
    "            for row in reader:\n",
    "                if row:  # avoid blank lines\n",
    "                    row_index += 1\n",
    "                    columns = [row[7]]\n",
    "                    data.append(columns)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#to separate threads with no title into posts\n",
    "def no_title(temp, all):\n",
    "    if (temp[0][0:2]=='{{') and (('documentation' in temp[0]) or ('Documentation' in temp[0])):\n",
    "        a=1 \n",
    "    else:\n",
    "        if ('#REDIRECT' not in temp[0]):\n",
    "            disc_dict={}\n",
    "            if '(UTC)' in temp[0]:\n",
    "                posts_lst=[]\n",
    "                disc_dict['subject']='No subject'\n",
    "                posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',temp[0])\n",
    "                for i in range(0,len(posts)-1, 2):\n",
    "                    posts_lst.append(posts[i]+posts[i+1])\n",
    "    \n",
    "                disc_dict['thread']=posts_lst\n",
    "                all.append(disc_dict) \n",
    "    return all\n",
    "\n",
    "\n",
    "#to separate threads with titles into posts\n",
    "def sep_posts(titles, temp, all):\n",
    "    for j in range(len(temp)):\n",
    "        if (temp[j] in titles):\n",
    "            disc_dict={}\n",
    "            if (j+1)==(len(temp)):\n",
    "                disc_dict['subject']=temp[j]\n",
    "                disc_dict['thread']=\"\"\n",
    "                all.append(disc_dict)\n",
    "            elif '(UTC)' in temp[j+1]:\n",
    "                posts_lst=[]\n",
    "                disc_dict['subject']=temp[j]\n",
    "                posts=re.split(r'(\\d\\d:\\d\\d, \\d+ \\w+ \\d\\d\\d\\d \\(UTC\\))',temp[j+1])\n",
    "                for i in range(0,len(posts)-1, 2):\n",
    "                    posts_lst.append(posts[i]+posts[i+1])\n",
    "    \n",
    "                disc_dict['thread']=posts_lst\n",
    "                all.append(disc_dict)\n",
    "    return all\n",
    "            \n",
    "\n",
    "#function to create a file with separate threads and separate posts for the discussion page\n",
    "def separate_discussions(file_name, new_file_path, name):\n",
    "\n",
    "    data = import_csv(file_name)#call the csv data\n",
    "    last_row = data[-1]#take the last row including all threads in the discussion page\n",
    "\n",
    "    titles = re.findall('==(.*)==', last_row[0])#find the titles in the discussion  \n",
    "    titles=[s.replace('=', '') for s in titles]\n",
    "    \n",
    "    #titles=list(set(titles)) \n",
    "    temp= last_row[0].split('==')\n",
    "    temp = [x for x in temp if x != '']\n",
    "    temp=[s.strip('=') for s in temp]\n",
    "    \n",
    "    all=[]\n",
    "    if titles==[]:\n",
    "        all=no_title(temp, all) \n",
    "    else:\n",
    "        all=no_title(temp, all)\n",
    "        all=sep_posts(titles, temp, all)\n",
    "\n",
    "    with open(str(new_file_path)+str(name[:-4])+'.json', \"w\") as outfile:\n",
    "        json_object = json.dumps(all,indent=4)                \n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path='JSON_FILE/'\n",
    "initial_folder_path='CSV_FILE/'\n",
    "filenames_folder='LIST_OF_CSV_FILES_TO_PROCESS/'\n",
    "\n",
    "\n",
    "#=======================================================================\n",
    "#========= ------->>>> R U N <<<<-------- ==============================\n",
    "\n",
    "#create a folder to save the new csv files with the edges (two columns with usernames that talk in the same talk pages)            \n",
    "# if not os.path.exists(new_folder_name):\n",
    "#         os.mkdir(new_folder_name)         \n",
    "\n",
    "#read the name of the csv files in the TP_csv_ folder  \n",
    "file1 = open(filenames_folder, 'r') \n",
    "Lines = file1.readlines() \n",
    "\n",
    "\n",
    "\n",
    "#call the csv files from the TP_csv_ folder\n",
    "for i in Lines:\n",
    "    #extarct the space before and after the string name\n",
    "    name=\"\".join(i.split())\n",
    "    print(\"start: \" +str(name))\n",
    "    #check if the csv is empty\n",
    "    if os.stat(str(initial_folder_path)+str(name)).st_size == 0:\n",
    "        continue\n",
    "    #call the function to find the usernames and create the edges\n",
    "    list_usernames=separate_discussions(str(initial_folder_path)+str(name),new_file_path , name)\n",
    "    print(\"finish: \" +str(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f38612211a5ac91abaa265d09091e9511deaf737e8b839893735c46e6969015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
